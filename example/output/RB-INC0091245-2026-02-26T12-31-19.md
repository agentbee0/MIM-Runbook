# Major Incident Runbook ‚Äî INC0091245
> Generated: Thu, 26 Feb 2026 12:31:19 GMT
> Incident Commander: Daniel Ortega

---
## 1. Incident Summary Banner

| Field | Value |
| --- | --- |
| Incident Number | INC0091245 |
| Title | Multi-Region Network Outage ‚Äî BGP Route Withdrawal Causing Global Connectivity Loss |
| Severity | 1 - Critical |
| Priority | 1 - Critical |
| State | In Progress |
| Affected Service | Global Application Network ‚Äî All Customer-Facing Traffic |
| Affected CI / Resource | core-router-edge-01 |
| Environment | Production (global (us-east-1, eu-west-1, ap-southeast-1)) |
| Category | Network / BGP / Routing |
| Assigned To | Network Operations |
| Assignment Group | NOC ‚Äî Tier 3 Network Engineering |
| Reported By | PagerDuty Auto-Alert (BGP peer down ‚Äî 3 simultaneous peers) |
| Opened At | 2026-02-26 09:14:00 UTC |
| Detected At | 2026-02-26 09:12:37 UTC |
| Change Related | YES ‚Äî investigate recent changes |
| Related Incident | None |

**Business Impact**

All inbound customer traffic to production services is failing. 100% packet loss from external clients to all three production regions. Estimated 45,000 active sessions dropped. Revenue impact ~$120K/minute. B2B SLA breach imminent for Tier-1 enterprise customers. Brand and regulatory exposure if outage exceeds 30 minutes.


> üî¥ **CRITICAL**: ‚ö†Ô∏è  This is a Severity 1 - Critical incident. Every minute of delay costs revenue and customer trust. Follow this runbook from top to bottom without skipping steps.

**Incident Description**

At 09:12:37 UTC PagerDuty fired a P1 alert: "BGP peer down ‚Äî 3 peers on core-router-edge-01".
Within 90 seconds all three transit BGP sessions (AS174 Cogent, AS6461 Zayo, AS3356 Lumen)
dropped. The autonomous system (AS64512) lost all upstream route advertisements.

Symptoms observed:
- 100% external packet loss to all production VIPs (load balancers, CDN origins)
- Internal east-west traffic between services is unaffected
- No hardware or interface errors on core-router-edge-01
- BGP daemon (FRRouTing 9.1) shows all peers in IDLE state
- syslog on core-router-edge-01 shows: "NOTIFICATION received from peer AS174: HOLD TIMER EXPIRED"
- Change freeze window is NOT active; a BGP community configuration change was pushed
  by network-eng at 09:05 UTC (7 minutes before the outage) ‚Äî change ticket CHG0034891

Possible cause: misconfigured BGP route-map applying a REJECT ALL policy to outbound
advertisements or modifying MED/LOCAL_PREF values that triggered peer resets.

No DDoS signature detected. CDN edge nodes (Cloudflare) show normal traffic in their
scrubbing centres ‚Äî issue is origin-side.


---
## 2. Immediate Triage Checklist (First 5 Minutes)

> üü° **WARNING**: Complete all steps in this section within 5 minutes of picking up the alert. Do not investigate root cause yet ‚Äî your goal is to confirm, assess, and mobilise.

**Step 1:** **T+0 ‚Äî Confirm the alert is genuine.**

Check that core-router-edge-01 is actually unreachable or degraded. Do NOT assume the monitoring tool is always right ‚Äî false positives happen.

Run: `ping core-router-edge-01` and `curl -I https://core-router-edge-01/health` (or equivalent health endpoint)

If the CI responds normally ‚Üí the alert may be a false positive. Acknowledge in monitoring and add a note to INC0091245 before standing down.

**Step 2:** **T+1 ‚Äî Check the monitoring dashboard.**

Open the primary monitoring dashboard for 'Global Application Network ‚Äî All Customer-Facing Traffic'. Look for:
‚Ä¢ Red/critical alerts in the last 15 minutes
‚Ä¢ Any correlated alerts (DB + App + Network firing together suggests a larger blast radius)
‚Ä¢ Comparison to baseline from same time yesterday

**Step 3:** **T+2 ‚Äî Assess blast radius.**

Answer these questions before joining the bridge:
‚Ä¢ How many users/customers are affected? (check active sessions, error rates)
‚Ä¢ What downstream services depend on 'core-router-edge-01'?
‚Ä¢ Is this isolated to one region (global (us-east-1, eu-west-1, ap-southeast-1)) or multi-region?
‚Ä¢ Are any other Sev1/Sev2 incidents currently open that could be related?

**Step 4:** **T+3 ‚Äî Open the incident Slack channel.**

Create channel: #incinc0091245

Paste this message as the first post:

```
üö® INCIDENT OPEN: INC0091245
Severity: 1 - Critical
Service: Global Application Network ‚Äî All Customer-Facing Traffic
CI: core-router-edge-01
Impact: All inbound customer traffic to production services is failing. 100% packet loss from external clients to all three production regions. Estimated 45,000 active sessions dropped. Revenue impact ~$120K/minute. B2B SLA breach imminent for Tier-1 enterprise customers. Brand and regulatory exposure if outage exceeds 30 minutes.

Bridge: https://corp.zoom.us/j/82345678901?pwd=WarRoom2026!
Runbook: [paste link to this document]
IC: Daniel Ortega (@daniel.ortega)
```

**Step 5:** **T+4 ‚Äî Join the Zoom bridge call.**

Bridge URL: https://corp.zoom.us/j/82345678901?pwd=WarRoom2026!
Dial-in: +1-669-900-6833 PIN: 987654#

When you join, say:
> "This is [YOUR NAME], I'm the on-call engineer. I'm declaring this a Sev1 for Global Application Network ‚Äî All Customer-Facing Traffic. Incident number INC0091245. Daniel Ortega is the Incident Commander. I'll begin triage while the team joins."

Record the exact time you joined.

**Step 6:** **T+5 ‚Äî Page immediate-notify stakeholders.**

The following people must be notified RIGHT NOW (use phone if Slack is unresponsive):

| Name | Role | Slack | Phone |
| --- | --- | --- | --- |
| [IC_NAME] | Incident Commander | [IC_SLACK] | [IC_PHONE] |
| [TECH_LEAD_NAME] | Technical Lead | [TECH_SLACK] | [TECH_PHONE] |
| [COMMS_LEAD_NAME] | Communications Lead | [COMMS_SLACK] | [COMMS_PHONE] |

---
## 3. Communication Plan

**Communication Owner**: Carlos Mendez (@carlos.mendez)

**Update Cadence**: Status updates every **30 minutes** until Sev1 is resolved, then final resolution email.

### Stakeholder Role Map

| Name | Role | Notify When | Method | Email |
| --- | --- | --- | --- | --- |
| Daniel Ortega | Incident Commander | Immediately (T+0) | Phone + Slack + Email | d.ortega@corp.example.com |
| Fatima Al-Hassan | Technical Lead | Immediately (T+0) | Phone + Slack + Email | f.alhassan@corp.example.com |
| Carlos Mendez | Communications Lead | Immediately (T+0) | Phone + Slack + Email | c.mendez@corp.example.com |
| Aisha Nwosu | On-Call Engineer | Immediately (T+0) | Phone + Slack + Email | a.nwosu@corp.example.com |
| Wei Zhang | On-Call Engineer | Immediately (T+0) | Phone + Slack + Email | w.zhang@corp.example.com |
| NOC Bridge Coordinator | NOC Lead | Immediately (T+0) | Phone + Slack + Email | noc-oncall@corp.example.com |
| Sandra Kowalski | Customer Impact Lead | T+30 or if not resolved | Slack + Email | s.kowalski@corp.example.com |
| Ravi Krishnamurthy | Product Lead | T+30 or if not resolved | Slack + Email | r.krishnamurthy@corp.example.com |
| Ingrid Larsson | Security Lead | T+30 or if not resolved | Slack + Email | i.larsson@corp.example.com |
| Marcus Webb | Legal and Compliance Lead | T+30 or if not resolved | Slack + Email | m.webb@corp.example.com |
| Eleanor Tran | Executive Sponsor | T+60 or if escalated | Slack + Email | e.tran@corp.example.com |
| Bernard Osei | Executive Sponsor | T+60 or if escalated | Slack + Email | b.osei@corp.example.com |
| Helena Vasquez | Executive Sponsor | T+60 or if escalated | Slack + Email | h.vasquez@corp.example.com |

### Slack Communication Steps

**Step 1:** Create Slack channel: **#inc0091245** (format: `/incident-channel INC0091245` or create manually)

**Step 2:** Invite the following people to the channel:
@daniel.ortega @fatima.alhassan @carlos.mendez @aisha.nwosu @wei.zhang @noc-oncall

**Step 3:** Pin the Incident Summary message at the top of the channel so all joiners see it immediately.

**Step 4:** Post a status update in the channel every 30 minutes, even if there is nothing new to report. Silence breeds rumour.

### Email Templates

Six ready-to-send email templates are provided below ‚Äî one for each phase of the incident. Copy-paste the template, fill in the bracketed placeholders [LIKE THIS], and send.

### üìß Email Template Initial Incident Notification (T+0 (send immediately upon declaring Sev1))
**Subject**: `[SEV1 ‚Äî ACTIVE] INC0091245: Global Application Network ‚Äî All Customer-Facing Traffic ‚Äî Multi-Region Network Outage ‚Äî BGP Route Withdrawal Causing Global Connectivity Loss`
**To**: d.ortega@corp.example.com; f.alhassan@corp.example.com; c.mendez@corp.example.com; a.nwosu@corp.example.com; w.zhang@corp.example.com; noc-oncall@corp.example.com
**CC**: s.kowalski@corp.example.com; r.krishnamurthy@corp.example.com; i.larsson@corp.example.com; m.webb@corp.example.com

```
Team,

We are declaring a Severity 1 incident affecting Global Application Network ‚Äî All Customer-Facing Traffic.

INCIDENT DETAILS
================
Incident Number : INC0091245
Severity        : 1 - Critical
Priority        : 1 - Critical
Affected Service: Global Application Network ‚Äî All Customer-Facing Traffic
Affected CI     : core-router-edge-01
Environment     : Production (global (us-east-1, eu-west-1, ap-southeast-1))
Detected At     : 2026-02-26 09:12:37 UTC UTC
Incident Commander: Daniel Ortega ‚Äî +1-415-555-0101

BUSINESS IMPACT
===============
All inbound customer traffic to production services is failing. 100% packet loss from external clients to all three production regions. Estimated 45,000 active sessions dropped. Revenue impact ~$120K/minute. B2B SLA breach imminent for Tier-1 enterprise customers. Brand and regulatory exposure if outage exceeds 30 minutes.


CURRENT STATUS
==============
We have declared a Sev1 and are mobilising the response team. Investigation is starting now.

WAR ROOM
========
Zoom Bridge : https://corp.zoom.us/j/82345678901?pwd=WarRoom2026!
Slack Channel: #incinc0091245

All Level 1 stakeholders, please join the bridge immediately.

Next update: T+30 minutes or sooner if there is a significant development.

-- Carlos Mendez
```

### üìß Email Template War Room / Bridge Established (T+5 (once bridge call is open and team is assembling))
**Subject**: `[SEV1 ‚Äî WAR ROOM OPEN] INC0091245: Bridge call is live ‚Äî join now`
**To**: d.ortega@corp.example.com; f.alhassan@corp.example.com; c.mendez@corp.example.com; a.nwosu@corp.example.com; w.zhang@corp.example.com; noc-oncall@corp.example.com
**CC**: s.kowalski@corp.example.com; r.krishnamurthy@corp.example.com; i.larsson@corp.example.com; m.webb@corp.example.com

```
Team,

The incident bridge for INC0091245 is now active.

BRIDGE DETAILS
==============
Zoom URL  : https://corp.zoom.us/j/82345678901?pwd=WarRoom2026!
Dial-In   : +1-669-900-6833 PIN: 987654#
Slack     : #incinc0091245

ON THE CALL NOW
===============
Daniel Ortega ‚Äî Incident Commander
[NAMES OF ENGINEERS ON CALL ‚Äî update as people join]

CURRENT WORKING HYPOTHESIS
===========================
[UPDATE THIS ‚Äî e.g., "We believe the Oracle cluster primary node has crashed. Investigating logs now."]

WHO WE NEED
===========
If you are a NOC ‚Äî Tier 3 Network Engineering engineer and are NOT on the call, please join immediately.

Next update: T+30 minutes.

-- Carlos Mendez
```

### üìß Email Template 30-Minute Status Update (T+30 (and every 30 minutes thereafter until resolved))
**Subject**: `[SEV1 ‚Äî T+[ELAPSED]MIN UPDATE] INC0091245: Status Update ‚Äî [CURRENT_STATUS_HEADLINE]`
**To**: d.ortega@corp.example.com; f.alhassan@corp.example.com; c.mendez@corp.example.com; a.nwosu@corp.example.com; w.zhang@corp.example.com; noc-oncall@corp.example.com; s.kowalski@corp.example.com; r.krishnamurthy@corp.example.com; i.larsson@corp.example.com; m.webb@corp.example.com
**CC**: e.tran@corp.example.com; b.osei@corp.example.com; h.vasquez@corp.example.com

```
Team,

Status update for INC0091245 as of [CURRENT_TIME] UTC (T+[ELAPSED] minutes).

CURRENT STATUS
==============
[REPLACE WITH: "Investigating" / "Root cause identified" / "Mitigation in progress" / "Monitoring after fix"]

ACTIONS TAKEN SO FAR
====================
[TIMESTAMP] [ACTION TAKEN] ‚Äî [NAME]
[TIMESTAMP] [ACTION TAKEN] ‚Äî [NAME]
[ADD MORE ROWS AS NEEDED]

CURRENT WORKING HYPOTHESIS
===========================
[REPLACE WITH YOUR CURRENT ROOT CAUSE HYPOTHESIS AND CONFIDENCE LEVEL]

NEXT STEPS
==========
[ACTION 1] ‚Äî Owner: [NAME], ETA: [TIME]
[ACTION 2] ‚Äî Owner: [NAME], ETA: [TIME]

ESTIMATED RESOLUTION
====================
[PROVIDE AN ETA OR STATE "ETA UNKNOWN ‚Äî next update in 30 minutes"]

Bridge is still active: https://corp.zoom.us/j/82345678901?pwd=WarRoom2026!
Slack: #incinc0091245

-- Carlos Mendez
```

### üìß Email Template Mitigation In Progress (T+[X] (send when a containment action is being executed))
**Subject**: `[SEV1 ‚Äî MITIGATION ACTIVE] INC0091245: Containment action underway for Global Application Network ‚Äî All Customer-Facing Traffic`
**To**: d.ortega@corp.example.com; f.alhassan@corp.example.com; c.mendez@corp.example.com; a.nwosu@corp.example.com; w.zhang@corp.example.com; noc-oncall@corp.example.com; s.kowalski@corp.example.com; r.krishnamurthy@corp.example.com; i.larsson@corp.example.com; m.webb@corp.example.com
**CC**: e.tran@corp.example.com; b.osei@corp.example.com; h.vasquez@corp.example.com

```
Team,

We are now executing a containment action for INC0091245.

CONTAINMENT ACTION
==================
Action         : [DESCRIBE THE ACTION, e.g., "Failing over Oracle cluster to standby node"]
Executed By    : [NAME]
Start Time     : [TIMESTAMP] UTC
Expected Impact: [e.g., "Brief 30-60 second connection interruption during failover"]
Expected ETA   : Service should begin recovering within [X] minutes

RISK ASSESSMENT
===============
Risk Level  : [High / Medium / Low]
Rollback Plan: [HOW TO ROLL BACK IF THIS ACTION MAKES THINGS WORSE]

IF THIS ACTION FAILS
====================
Escalation to: Sandra Kowalski, Ravi Krishnamurthy, Ingrid Larsson, Marcus Webb
Vendor support: [VENDOR_NAME] ‚Äî open ticket at [SUPPORT_URL]

We will send a follow-up update within 15 minutes of execution.

Bridge: https://corp.zoom.us/j/82345678901?pwd=WarRoom2026!
Slack: #incinc0091245

-- Carlos Mendez
```

### üìß Email Template Service Restored (T+[X] (send within 10 minutes of confirming service is restored))
**Subject**: `[SEV1 ‚Äî RESOLVED ‚úÖ] INC0091245: Global Application Network ‚Äî All Customer-Facing Traffic has been restored`
**To**: d.ortega@corp.example.com; f.alhassan@corp.example.com; c.mendez@corp.example.com; a.nwosu@corp.example.com; w.zhang@corp.example.com; noc-oncall@corp.example.com; s.kowalski@corp.example.com; r.krishnamurthy@corp.example.com; i.larsson@corp.example.com; m.webb@corp.example.com; e.tran@corp.example.com; b.osei@corp.example.com; h.vasquez@corp.example.com

```
Team,

Global Application Network ‚Äî All Customer-Facing Traffic has been restored.

RESOLUTION SUMMARY
==================
Incident Number  : INC0091245
Affected Service : Global Application Network ‚Äî All Customer-Facing Traffic
Incident Opened  : 2026-02-26 09:12:37 UTC UTC
Service Restored : [RESOLVED_TIMESTAMP] UTC
Total Duration   : [X hours Y minutes]

ROOT CAUSE (PRELIMINARY)
========================
[REPLACE WITH ROOT CAUSE SUMMARY ‚Äî 2-3 sentences. E.g., "An Oracle RAC primary node failure caused by shared pool memory exhaustion triggered by a runaway query introduced in release v2.4.1 (deployed at 13:45 UTC). Failover to the standby node restored service."]

REMEDIATION APPLIED
===================
[ACTION 1 ‚Äî e.g., "Failover to Oracle standby node prod-oracle-02 at [TIME] UTC"]
[ACTION 2 ‚Äî e.g., "Runaway query identified and index added to prevent recurrence"]

MONITORING STATUS
=================
‚úÖ Synthetic monitor: GREEN
‚úÖ Error rate: Within normal range
‚úÖ P99 latency: Normal
‚úÖ Customer-facing checkout: Functioning

NEXT STEPS
==========
1. Post-Incident Review (PIR) will be scheduled within 5 business days
2. Root cause analysis in progress
3. Monitoring continues for the next 2 hours

Thank you to everyone who responded ‚Äî especially Daniel Ortega and NOC ‚Äî Tier 3 Network Engineering.

Full runbook and action log attached.

-- Carlos Mendez
```

### üìß Email Template Incident Closed + PIR Invitation (T+24h (within 24 hours of resolution, once PIR is scheduled))
**Subject**: `[INC0091245 CLOSED] Post-Incident Review scheduled ‚Äî Multi-Region Network Outage ‚Äî BGP Route Withdrawal Causing Global Connectivity Loss`
**To**: d.ortega@corp.example.com; f.alhassan@corp.example.com; c.mendez@corp.example.com; a.nwosu@corp.example.com; w.zhang@corp.example.com; noc-oncall@corp.example.com; s.kowalski@corp.example.com; r.krishnamurthy@corp.example.com; i.larsson@corp.example.com; m.webb@corp.example.com
**CC**: e.tran@corp.example.com; b.osei@corp.example.com; h.vasquez@corp.example.com

```
Team,

INC0091245 has been formally closed. The Post-Incident Review (PIR) has been scheduled.

INCIDENT SUMMARY
================
Incident : INC0091245
Service  : Global Application Network ‚Äî All Customer-Facing Traffic
Severity : 1 - Critical
Duration : [X hours Y minutes]
Customers Affected: [NUMBER / PERCENTAGE]
Business Impact: All inbound customer traffic to production services is failing. 100% packet loss from external clients to all three production regions. Estimated 45,000 active sessions dropped. Revenue impact ~$120K/minute. B2B SLA breach imminent for Tier-1 enterprise customers. Brand and regulatory exposure if outage exceeds 30 minutes.


PIR DETAILS
===========
Date    : [PIR_DATE]
Time    : [PIR_TIME] UTC
Duration: 60 minutes
Link    : [ZOOM_OR_CALENDAR_LINK]

Attendees (required):
Daniel Ortega ‚Äî Incident Commander
[TECHNICAL LEAD NAME]
[COMMS LEAD NAME]
[ADD OTHER KEY RESPONDERS]

PRE-READ MATERIALS
==================
Please review these before the PIR:
1. Incident runbook: [LINK TO .docx]
2. Action tracker / timeline: [LINK TO .xlsx]
3. Preliminary root cause document: [LINK]

PIR AGENDA
==========
1. Timeline walkthrough (15 min)
2. Root cause deep dive (20 min)
3. What went well (10 min)
4. What to improve (10 min)
5. Action item assignment (5 min)

If you cannot attend, please send your notes to d.ortega@corp.example.com before the meeting.

-- Carlos Mendez
```

---
## 4. Diagnosis & Investigation Steps

**Owner**: Fatima Al-Hassan (@fatima.alhassan)

**Goal of this section**: Understand WHAT is broken and WHY, so you can take the right containment action. Work through steps in order. Document every finding in the Slack channel as you go.

> üîµ **INFO**: For every check below: state what you expected vs. what you found. 'Good' means the result matches expected baseline. 'Bad' means it deviates and needs investigation.

**Step 1:** **Confirm reachability from multiple vantage points.**

```
# From your workstation:
ping -c 5 core-router-edge-01
traceroute core-router-edge-01
mtr --report core-router-edge-01

# From another host in the same region:
ssh jump-host "ping -c 5 core-router-edge-01"
ssh jump-host "curl -o /dev/null -s -w '%{http_code}' https://core-router-edge-01/health"
```

‚úÖ **Good**: Reachable from all vantage points, <10ms latency
‚ùå **Bad**: Unreachable from some/all ‚Üí packet loss indicates routing or firewall issue
‚ö° **Decision**: If reachable from internal but not external ‚Üí check firewall/CDN (Step 3)

**Step 2:** **Check routing tables and BGP status.**

```
# Check routing:
ip route show
netstat -rn

# For BGP environments (check with network team):
show ip bgp summary   # (on router/switch)
show ip route bgp

# Check DNS resolution:
dig core-router-edge-01
nslookup core-router-edge-01
```

‚úÖ **Good**: Correct routes present, BGP peers UP, DNS resolves correctly
‚ùå **Bad**: Missing routes, BGP peer DOWN, DNS returns wrong IP ‚Üí network team escalation required

**Step 3:** **Check firewall and security group rules.**

Verify no rule was recently changed that blocks traffic to/from core-router-edge-01:
```
# Linux firewall:
sudo iptables -L -n -v | head -50
sudo firewall-cmd --list-all

# AWS Security Groups (if applicable):
aws ec2 describe-security-groups --group-ids [SG_ID] --query 'SecurityGroups[*].IpPermissions'
```

‚ö° **Decision**: If a rule was changed in last 4 hours ‚Üí revert the change (Section 5)

**Step 4:** **Check CDN and load balancer health.**

Verify CDN edge nodes are serving traffic and not returning errors:
```
curl -I -H "Host: Global Application Network ‚Äî All Customer-Facing Traffic" https://[CDN_EDGE_IP]/health
curl -I https://Global Application Network ‚Äî All Customer-Facing Traffic/health
```

Check load balancer health page in your cloud console or management tool. Look for:
‚Ä¢ Backend instance health checks failing
‚Ä¢ SSL certificate expiry (check expiry date)
‚Ä¢ Origin connection errors

**What did network diagnosis reveal?**

- If **BGP peer down or routing change** ‚Üí Section 5, Step 1: Revert routing change or failover to backup link
- If **Firewall rule blocking traffic** ‚Üí Section 5, Step 2: Revert firewall rule change
- If **DNS misconfiguration** ‚Üí Section 5, Step 3: Fix DNS record and force propagation
- If **CDN/Load balancer issue** ‚Üí Section 5, Step 4: Bypass CDN or failover load balancer
- If **Unresolved ‚Äî no root cause found** ‚Üí Escalate to Senior Network Engineer and vendor (Section 6)

### Root Cause Hypothesis Log

Before moving to containment, post your working hypothesis in the Slack channel:

```
We believe the root cause is: [YOUR HYPOTHESIS]
Evidence supporting this: [LIST EVIDENCE]
Evidence against this: [CONTRADICTORY FINDINGS]
Confidence level: High / Medium / Low
Recommended next action: [SPECIFIC ACTION]
```

---
## 5. Containment & Mitigation Actions

**Owner**: Fatima Al-Hassan

**Rule**: Before executing any containment action, announce it in inc0091245 Slack channel:
> "About to execute: [ACTION]. Expected impact: [IMPACT]. Rollback plan: [ROLLBACK]. Approvals: [NAMES]"

For **irreversible or high-risk actions**, obtain verbal approval from the Incident Commander and document it in the Slack channel.

> üü° **WARNING**: CAB Emergency Approval: Some actions below (marked ‚ö†Ô∏è CAB) require Change Advisory Board emergency approval. To get fast approval: (1) Call the CAB emergency line at [CAB_PHONE], (2) State incident number and requested change, (3) Get verbal approval from the CAB chair, (4) Log the change in your ITSM tool immediately after.

**Step 1:** **Revert the routing change / re-advertise the route.**

‚ö†Ô∏è CAB required.

```
# Rollback network change via your network management tool
# Or re-advertise the withdrawn BGP route
# Specific commands depend on your network vendor ‚Äî contact your network team
```

**Validate**: Run `traceroute core-router-edge-01` from multiple vantage points.

**Step 2:** **Revert firewall rule change.**

```
# Linux iptables ‚Äî remove the blocking rule:
sudo iptables -D INPUT -s [BLOCKED_IP] -j DROP

# AWS Security Group ‚Äî restore the inbound rule:
aws ec2 authorize-security-group-ingress \
  --group-id [SG_ID] \
  --protocol tcp --port [PORT] --cidr [CIDR]
```

**Step 3:** **Bypass CDN and route traffic directly to origin.**

Update DNS to point directly to origin load balancer (bypasses CDN caching and edge issues):
```
# AWS Route 53 ‚Äî update record to point to ALB:
aws route53 change-resource-record-sets --hosted-zone-id [ZONE_ID] \
  --change-batch file://failover-dns.json
```

**Monitor**: DNS propagation takes 1-5 minutes. Monitor via: `watch -n 10 "dig core-router-edge-01 @8.8.8.8"`

**Rollback**: Revert DNS record back to CDN CNAME once CDN issue is resolved.

---
## 6. Escalation Matrix

Use this matrix to escalate if the incident is not resolved within the stated time window. **Do not wait until the time is up** ‚Äî if you are not making progress, escalate sooner.

| Time Elapsed | Action | Escalate To | Contact Method |
| --- | --- | --- | --- |
| T+15 (not contained) | Escalate to Technical Lead + IC if not already engaged | Daniel Ortega, Fatima Al-Hassan, Carlos Mendez, Aisha Nwosu, Wei Zhang, NOC Bridge Coordinator | Phone + Slack |
| T+30 (not contained) | Escalate to VP/Senior Management; consider vendor support | Sandra Kowalski, Ravi Krishnamurthy, Ingrid Larsson, Marcus Webb | Phone + Email |
| T+60 (not contained) | Escalate to C-Suite; open P1 case with relevant vendor | Eleanor Tran, Bernard Osei, Helena Vasquez | Phone |
| T+120 (still unresolved) | Consider declaring Major Incident; invoke DR plan; all hands | All stakeholders + DRI team | Emergency all-hands bridge |

### Individual Escalation Contacts

| Name | Role | Escalation Level | Phone | Email | Slack |
| --- | --- | --- | --- | --- | --- |
| Daniel Ortega | Incident Commander | Level 1 ‚Äî Immediate | +1-415-555-0101 | d.ortega@corp.example.com | @daniel.ortega |
| Fatima Al-Hassan | Technical Lead | Level 1 ‚Äî Immediate | +1-415-555-0102 | f.alhassan@corp.example.com | @fatima.alhassan |
| Carlos Mendez | Communications Lead | Level 1 ‚Äî Immediate | +1-415-555-0103 | c.mendez@corp.example.com | @carlos.mendez |
| Aisha Nwosu | On-Call Engineer | Level 1 ‚Äî Immediate | +1-415-555-0104 | a.nwosu@corp.example.com | @aisha.nwosu |
| Wei Zhang | On-Call Engineer | Level 1 ‚Äî Immediate | +1-415-555-0105 | w.zhang@corp.example.com | @wei.zhang |
| NOC Bridge Coordinator | NOC Lead | Level 1 ‚Äî Immediate | +1-415-555-0199 | noc-oncall@corp.example.com | @noc-oncall |
| Sandra Kowalski | Customer Impact Lead | Level 2 ‚Äî T+30 | +1-415-555-0201 | s.kowalski@corp.example.com | @sandra.kowalski |
| Ravi Krishnamurthy | Product Lead | Level 2 ‚Äî T+30 | +1-415-555-0202 | r.krishnamurthy@corp.example.com | @ravi.krishnamurthy |
| Ingrid Larsson | Security Lead | Level 2 ‚Äî T+30 | +1-415-555-0203 | i.larsson@corp.example.com | @ingrid.larsson |
| Marcus Webb | Legal and Compliance Lead | Level 2 ‚Äî T+30 | +1-415-555-0204 | m.webb@corp.example.com | @marcus.webb |
| Eleanor Tran | Executive Sponsor | Level 3 ‚Äî T+60+ | +1-415-555-0301 | e.tran@corp.example.com | @eleanor.tran |
| Bernard Osei | Executive Sponsor | Level 3 ‚Äî T+60+ | +1-415-555-0302 | b.osei@corp.example.com | @bernard.osei |
| Helena Vasquez | Executive Sponsor | Level 3 ‚Äî T+60+ | +1-415-555-0303 | h.vasquez@corp.example.com | @helena.vasquez |

### Vendor / Third-Party Escalation

| Vendor | Account Number | Support URL | Phone | Severity to Declare |
| --- | --- | --- | --- | --- |
| AWS Support | AWS-112233445566 | https://console.aws.amazon.com/support/home | +1-888-280-4331 | P1 - Production System Down |
| Cloudflare Enterprise | CF-ENT-998877 | https://dash.cloudflare.com/support | +1-650-319-8930 | Business Critical |
| Stripe Support | STR-acct-1A2B3C4D5E6F | https://support.stripe.com | +1-888-963-8220 | P1 - Production Incident |
| Oracle Support | ORA-77665544 | https://support.oracle.com | +1-800-223-1711 | SEV1 - Production Down |
| PagerDuty Support | PD-ENT-CORP2026 | https://support.pagerduty.com | +1-415-990-9199 | Critical |

---
## 7. Resolution & Validation

**Owner**: Daniel Ortega

Before declaring the incident resolved, ALL of the following criteria must be met. Do not close the incident if any item is failing or unclear.

### Resolution Criteria Checklist

- [ ] Health endpoint returns 200 OK: `curl -I https://core-router-edge-01/health`

- [ ] Synthetic monitor for 'Global Application Network ‚Äî All Customer-Facing Traffic' is GREEN (check your monitoring tool)

- [ ] Error rate has returned to pre-incident baseline (check APM dashboard)

- [ ] P99 latency has returned to normal range

- [ ] Customer-facing checkout / primary user flow is functioning end-to-end

- [ ] Alerting rules are no longer firing

- [ ] On-call engineer has monitored for 10 minutes with no re-trigger

- [ ] Business stakeholder (Customer Impact Lead) has confirmed no open customer complaints

### Severity Downgrade Criteria

| Condition | Action |
| --- | --- |
| All health checks pass, no customer impact, stable for 10 min | Downgrade to Sev2 and continue monitoring |
| Partial restoration (some customers still affected) | Downgrade to Sev2, keep bridge open, continue investigation |
| Workaround in place (not fully resolved) | Downgrade to Sev2, open follow-up task for permanent fix |

### Closing the Bridge

**Step 1:** Announce in the Zoom bridge: "Service is restored. All health checks passing. We are closing the bridge. Thank you everyone."

**Step 2:** Post in Slack channel #inc0091245:

```
‚úÖ INCIDENT RESOLVED: INC0091245
Service: Global Application Network ‚Äî All Customer-Facing Traffic
Resolved At: [TIMESTAMP UTC]
Duration: [X hours Y minutes]
Root Cause (preliminary): [ROOT_CAUSE_HYPOTHESIS]
Action Taken: [SUMMARY_OF_FIX]
Next Steps: PIR to be scheduled within 5 business days
```

**Step 3:** Send the Service Restored email (Template 5 in Section 3).

**Step 4:** Stand down all escalated stakeholders ‚Äî send personal message or call to confirm they know the incident is over.

---
## 8. Post-Incident Handoff

**Owner**: Daniel Ortega

Complete this section before closing INC0091245 in ServiceNow. A well-documented post-incident record prevents the same incident from happening again.

### Documentation Before Closing

- [ ] Complete timeline of events documented (use Slack channel history + Excel Timeline sheet)

- [ ] All containment actions documented with exact timestamps and who performed them

- [ ] Root cause hypothesis documented (even if unconfirmed ‚Äî note confidence level)

- [ ] Customer impact quantified (number of users, duration, business cost)

- [ ] Any temporary workarounds noted with follow-up tasks created

- [ ] All follow-up Jira/ticket items created and assigned with due dates

### ServiceNow Resolution Update

Update ticket **INC0091245** in ServiceNow with the following information:

**Resolution Notes Template**:

```
RESOLUTION NOTES ‚Äî INC0091245

Incident opened: [OPENED_TIMESTAMP]
Service restored: [RESOLVED_TIMESTAMP]
Total duration: [X hours Y minutes]

Root Cause (Preliminary):
[DESCRIBE THE ROOT CAUSE - be specific, not generic. E.g., "Oracle RAC node prod-oracle-01 crashed due to ORA-04031 (shared pool memory exhaustion) triggered by an unoptimized query deployed in release v2.4.1 at 13:45 UTC"]

Containment Actions Taken:
1. [TIMESTAMP] [ACTION] ‚Äî performed by [NAME]
2. [TIMESTAMP] [ACTION] ‚Äî performed by [NAME]

Customer Impact:
- Duration of impact: [X hours Y minutes]
- Services affected: [LIST]
- Estimated users impacted: [NUMBER]

Follow-up Actions (link to tickets):
- [JIRA-XXX]: Root cause investigation
- [JIRA-XXX]: Monitoring/alerting improvements
- [JIRA-XXX]: Process improvements

PIR scheduled: [DATE] [TIME] with [ATTENDEES]
```

### Post-Incident Review (PIR) Ticket

Raise a PIR ticket (Problem record in ServiceNow) within 24 hours of resolution. Use the template below:

```
PIR TICKET ‚Äî INC0091245

Title: PIR: Multi-Region Network Outage ‚Äî BGP Route Withdrawal Causing Global Connectivity Loss
Category: Problem Management
Priority: 2 - High
Assigned To: Daniel Ortega

Description:
Post-Incident Review for INC0091245 ‚Äî Multi-Region Network Outage ‚Äî BGP Route Withdrawal Causing Global Connectivity Loss

Incident Summary:
- Severity: 1 - Critical
- Affected Service: Global Application Network ‚Äî All Customer-Facing Traffic
- Detected: [TIMESTAMP]
- Resolved: [TIMESTAMP]
- Duration: [X hours Y minutes]
- Impact: All inbound customer traffic to production services is failing. 100% packet loss from external clients to all three production regions. Estimated 45,000 active sessions dropped. Revenue impact ~$120K/minute. B2B SLA breach imminent for Tier-1 enterprise customers. Brand and regulatory exposure if outage exceeds 30 minutes.


Review Meeting:
- Date: [PIR_DATE] (within 5 business days)
- Duration: 60 minutes
- Attendees: Daniel Ortega, [TECH_LEAD], [COMMS_LEAD], [AFFECTED_TEAM_LEADS]

Agenda:
1. Timeline walkthrough (15 min)
2. Root cause analysis (20 min)
3. What went well (10 min)
4. What to improve (10 min)
5. Action items (5 min)

Pre-reads:
- This runbook: [LINK]
- Incident timeline (Excel Sheet 3): [LINK]
- Monitoring dashboard screenshots: [LINK]
```

### ServiceNow Ticket Closure Checklist

- [ ] Set State to "Resolved"

- [ ] Set Close Code to appropriate category (e.g., 'Software', 'Infrastructure', 'User Error')

- [ ] Fill in Resolution Notes (use template above)

- [ ] Link PIR Problem ticket in the Related Problems field

- [ ] Attach this runbook (.docx) and the action tracker (.xlsx) to the ticket

- [ ] Send Template 6 (PIR Invitation email) to all stakeholders

- [ ] Archive the Slack incident channel (do NOT delete ‚Äî retain for audit)
